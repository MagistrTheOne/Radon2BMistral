"""
Russian NLP Benchmark Suite for RADON
Comprehensive evaluation on Russian language tasks
"""

import os
import json
import time
import torch
import numpy as np
from typing import Dict, List, Any, Optional, Tuple
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset
import evaluate
from pathlib import Path


class RussianNLPSuite:
    """
    Comprehensive benchmark suite for Russian NLP tasks
    
    Tasks included:
    - Russian SuperGLUE
    - Russian Code Generation
    - Multilingual Translation (RU-EN)
    - Russian Text Classification
    - Russian Named Entity Recognition
    """
    
    def __init__(self, model_name: str = "MagistrTheOne/RadonSAI-Pretrained"):
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.results = {}
        
        # Load metrics
        self.accuracy_metric = evaluate.load("accuracy")
        self.bleu_metric = evaluate.load("bleu")
        self.rouge_metric = evaluate.load("rouge")
        self.f1_metric = evaluate.load("f1")
    
    def load_model(self):
        """Load RADON model and tokenizer"""
        print(f"Loading RADON model: {self.model_name}")
        
        try:
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                torch_dtype=torch.float16,
                device_map="auto"
            )
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            
            # Set pad token if not set
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            print("âœ… Model loaded successfully")
            return True
            
        except Exception as e:
            print(f"âŒ Failed to load model: {e}")
            return False
    
    def evaluate_russian_superglue(self) -> Dict[str, float]:
        """
        Evaluate on Russian SuperGLUE tasks
        """
        print("\nğŸ“Š Evaluating Russian SuperGLUE...")
        
        # Sample Russian SuperGLUE tasks
        tasks = {
            "russian_qa": {
                "question": "Ğ§Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğµ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ?",
                "context": "ĞœĞ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ - ÑÑ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ€Ğ°Ğ·Ğ´ĞµĞ» Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ°Ğ¼ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒÑÑ Ğ¸ Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· ÑĞ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.",
                "expected_answer": "Ğ¿Ğ¾Ğ´Ñ€Ğ°Ğ·Ğ´ĞµĞ» Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°"
            },
            "russian_sentiment": {
                "text": "Ğ­Ñ‚Ğ¾Ñ‚ Ñ„Ğ¸Ğ»ÑŒĞ¼ Ğ±Ñ‹Ğ» Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ¿Ğ¾Ñ‚Ñ€ÑÑĞ°ÑÑ‰Ğ¸Ğ¼!",
                "expected_label": "positive"
            },
            "russian_nli": {
                "premise": "Ğ¡Ğ¾Ğ±Ğ°ĞºĞ° Ğ±ĞµĞ¶Ğ¸Ñ‚ Ğ¿Ğ¾ Ğ¿Ğ°Ñ€ĞºÑƒ",
                "hypothesis": "Ğ–Ğ¸Ğ²Ğ¾Ñ‚Ğ½Ğ¾Ğµ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ½Ğ° ÑƒĞ»Ğ¸Ñ†Ğµ",
                "expected_label": "entailment"
            }
        }
        
        results = {}
        
        for task_name, task_data in tasks.items():
            print(f"  Testing {task_name}...")
            
            # Generate response
            prompt = self._create_prompt(task_name, task_data)
            response = self._generate_response(prompt)
            
            # Evaluate based on task type
            if task_name == "russian_qa":
                score = self._evaluate_qa(response, task_data["expected_answer"])
            elif task_name == "russian_sentiment":
                score = self._evaluate_sentiment(response, task_data["expected_label"])
            elif task_name == "russian_nli":
                score = self._evaluate_nli(response, task_data["expected_label"])
            
            results[task_name] = score
            print(f"    Score: {score:.3f}")
        
        avg_score = np.mean(list(results.values()))
        results["average"] = avg_score
        
        print(f"ğŸ“ˆ Russian SuperGLUE Average: {avg_score:.3f}")
        return results
    
    def evaluate_code_generation(self) -> Dict[str, float]:
        """
        Evaluate Russian code generation capabilities
        """
        print("\nğŸ’» Evaluating Code Generation...")
        
        code_tasks = {
            "python_function": {
                "prompt": "ĞĞ°Ğ¿Ğ¸ÑˆĞ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ½Ğ° Python Ğ´Ğ»Ñ ÑĞ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ ÑĞ¿Ğ¸ÑĞºĞ°:",
                "expected_keywords": ["def", "sort", "return"]
            },
            "russian_comments": {
                "prompt": "Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ¹ ĞºĞ»Ğ°ÑÑ Ğ½Ğ° Python Ñ Ñ€ÑƒÑÑĞºĞ¸Ğ¼Ğ¸ ĞºĞ¾Ğ¼Ğ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ¸ÑĞ¼Ğ¸:",
                "expected_keywords": ["class", "#", "def"]
            },
            "data_processing": {
                "prompt": "ĞĞ°Ğ¿Ğ¸ÑˆĞ¸ ĞºĞ¾Ğ´ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ñ€ÑƒÑÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ:",
                "expected_keywords": ["import", "pandas", "numpy"]
            }
        }
        
        results = {}
        
        for task_name, task_data in code_tasks.items():
            print(f"  Testing {task_name}...")
            
            response = self._generate_response(task_data["prompt"])
            score = self._evaluate_code_generation(response, task_data["expected_keywords"])
            
            results[task_name] = score
            print(f"    Score: {score:.3f}")
        
        avg_score = np.mean(list(results.values()))
        results["average"] = avg_score
        
        print(f"ğŸ“ˆ Code Generation Average: {avg_score:.3f}")
        return results
    
    def evaluate_translation(self) -> Dict[str, float]:
        """
        Evaluate Russian-English translation
        """
        print("\nğŸŒ Evaluating Translation...")
        
        translation_pairs = [
            {
                "russian": "ĞœĞ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ - ÑÑ‚Ğ¾ Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞµ Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¹",
                "english": "Machine learning is the future of technology"
            },
            {
                "russian": "Ğ˜ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸",
                "english": "Artificial intelligence helps solve complex problems"
            },
            {
                "russian": "ĞĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑĞµÑ‚Ğ¸ Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒÑÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ·Ğ³Ğ°",
                "english": "Neural networks mimic the work of the human brain"
            }
        ]
        
        results = {}
        
        for i, pair in enumerate(translation_pairs):
            print(f"  Testing translation {i+1}...")
            
            # Russian to English
            ru_to_en_prompt = f"ĞŸĞµÑ€ĞµĞ²ĞµĞ´Ğ¸ Ğ½Ğ° Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¸Ğ¹: {pair['russian']}"
            ru_to_en_response = self._generate_response(ru_to_en_prompt)
            ru_to_en_score = self._evaluate_translation(ru_to_en_response, pair["english"])
            
            # English to Russian
            en_to_ru_prompt = f"ĞŸĞµÑ€ĞµĞ²ĞµĞ´Ğ¸ Ğ½Ğ° Ñ€ÑƒÑÑĞºĞ¸Ğ¹: {pair['english']}"
            en_to_ru_response = self._generate_response(en_to_ru_prompt)
            en_to_ru_score = self._evaluate_translation(en_to_ru_response, pair["russian"])
            
            results[f"ru_to_en_{i+1}"] = ru_to_en_score
            results[f"en_to_ru_{i+1}"] = en_to_ru_score
            
            print(f"    RUâ†’EN: {ru_to_en_score:.3f}, ENâ†’RU: {en_to_ru_score:.3f}")
        
        avg_score = np.mean(list(results.values()))
        results["average"] = avg_score
        
        print(f"ğŸ“ˆ Translation Average: {avg_score:.3f}")
        return results
    
    def evaluate_long_context(self) -> Dict[str, float]:
        """
        Evaluate long context understanding in Russian
        """
        print("\nğŸ“š Evaluating Long Context...")
        
        # Create long Russian text
        long_text = """
        ĞœĞ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ€Ğ°Ğ·Ğ´ĞµĞ» Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² Ğ¸ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ±ĞµĞ· ÑĞ²Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¾Ğ½Ğ¸ Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑ‚ÑÑ Ğ½Ğ° Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ñ‹, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸Ğ· Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….

        ĞĞ±Ğ»Ğ°ÑÑ‚ÑŒ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‚ĞµÑĞ½Ğ¾ ÑĞ²ÑĞ·Ğ°Ğ½Ğ° Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ¾Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ¾Ğ². Ğ˜Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ñ‚ĞµĞ¾Ñ€Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.

        ĞœĞ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½Ğ¾Ğ³Ğ´Ğ° Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ÑÑ Ñ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ³Ğ´Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½ÑÑ ÑÑƒĞ±Ğ´Ğ¸ÑÑ†Ğ¸Ğ¿Ğ»Ğ¸Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ¼ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ğ° ĞºĞ°Ğº Ğ½ĞµĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ. ĞœĞ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ğ¾ ĞºĞ°Ğº Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ°.

        Ğ¡ÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒĞµÑ‚ Ñ‚Ñ€Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ° Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ· ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğº Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼.

        ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ· ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±ĞµĞ· Ğ¿Ğ¾Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ¾Ñ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ¹ ÑÑ€ĞµĞ´Ğ¾Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸.
        """
        
        questions = [
            "ĞšĞ°ĞºĞ¸Ğµ Ñ‚Ñ€Ğ¸ Ñ‚Ğ¸Ğ¿Ğ° Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑƒĞ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°ÑÑ‚ÑÑ Ğ² Ñ‚ĞµĞºÑÑ‚Ğµ?",
            "Ğ§Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼?",
            "ĞšĞ°Ğº Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ²ÑĞ·Ğ°Ğ½Ğ¾ Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ¾Ğ¹?"
        ]
        
        results = {}
        
        for i, question in enumerate(questions):
            print(f"  Testing question {i+1}...")
            
            prompt = f"ĞšĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚: {long_text}\n\nĞ’Ğ¾Ğ¿Ñ€Ğ¾Ñ: {question}\n\nĞÑ‚Ğ²ĞµÑ‚:"
            response = self._generate_response(prompt)
            
            # Simple evaluation based on keyword presence
            score = self._evaluate_qa(response, question)
            results[f"question_{i+1}"] = score
            
            print(f"    Score: {score:.3f}")
        
        avg_score = np.mean(list(results.values()))
        results["average"] = avg_score
        
        print(f"ğŸ“ˆ Long Context Average: {avg_score:.3f}")
        return results
    
    def _create_prompt(self, task_name: str, task_data: Dict) -> str:
        """Create prompt for specific task"""
        if task_name == "russian_qa":
            return f"ĞšĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚: {task_data['context']}\n\nĞ’Ğ¾Ğ¿Ñ€Ğ¾Ñ: {task_data['question']}\n\nĞÑ‚Ğ²ĞµÑ‚:"
        elif task_name == "russian_sentiment":
            return f"ĞĞ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸ Ñ‚Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ‚ĞµĞºÑÑ‚Ğ°: {task_data['text']}\n\nĞ¢Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ:"
        elif task_name == "russian_nli":
            return f"ĞŸĞ¾ÑÑ‹Ğ»ĞºĞ°: {task_data['premise']}\nĞ“Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·Ğ°: {task_data['hypothesis']}\n\nĞÑ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğµ:"
        else:
            return str(task_data)
    
    def _generate_response(self, prompt: str, max_length: int = 100) -> str:
        """Generate response using RADON model"""
        if self.model is None or self.tokenizer is None:
            return ""
        
        try:
            inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512)
            inputs = {k: v.to(self.model.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_length=inputs["input_ids"].shape[1] + max_length,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )
            
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            # Remove the original prompt from response
            if response.startswith(prompt):
                response = response[len(prompt):].strip()
            
            return response
            
        except Exception as e:
            print(f"Generation error: {e}")
            return ""
    
    def _evaluate_qa(self, response: str, expected: str) -> float:
        """Evaluate QA response"""
        # Simple keyword-based evaluation
        expected_words = set(expected.lower().split())
        response_words = set(response.lower().split())
        
        if not expected_words:
            return 0.0
        
        overlap = len(expected_words.intersection(response_words))
        return overlap / len(expected_words)
    
    def _evaluate_sentiment(self, response: str, expected: str) -> float:
        """Evaluate sentiment classification"""
        response_lower = response.lower()
        
        if expected == "positive":
            positive_words = ["Ğ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹", "Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¸Ğ¹", "Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¹", "Ğ¿Ğ¾Ñ‚Ñ€ÑÑĞ°ÑÑ‰Ğ¸Ğ¹", "Ğ·Ğ°Ğ¼ĞµÑ‡Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹"]
            return 1.0 if any(word in response_lower for word in positive_words) else 0.0
        elif expected == "negative":
            negative_words = ["Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹", "Ğ¿Ğ»Ğ¾Ñ…Ğ¾Ğ¹", "ÑƒĞ¶Ğ°ÑĞ½Ñ‹Ğ¹", "Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‚Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹"]
            return 1.0 if any(word in response_lower for word in negative_words) else 0.0
        else:
            return 0.5  # Neutral
    
    def _evaluate_nli(self, response: str, expected: str) -> float:
        """Evaluate NLI response"""
        response_lower = response.lower()
        
        if expected == "entailment":
            entailment_words = ["ÑĞ»ĞµĞ´ÑƒĞµÑ‚", "Ğ¿Ğ¾Ğ´Ñ€Ğ°Ğ·ÑƒĞ¼ĞµĞ²Ğ°ĞµÑ‚", "Ğ²Ğ»ĞµÑ‡ĞµÑ‚", "entailment"]
            return 1.0 if any(word in response_lower for word in entailment_words) else 0.0
        elif expected == "contradiction":
            contradiction_words = ["Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ñ€ĞµÑ‡Ğ¸Ñ‚", "contradiction", "Ğ½ĞµĞ²ĞµÑ€Ğ½Ğ¾"]
            return 1.0 if any(word in response_lower for word in contradiction_words) else 0.0
        else:
            return 0.5  # Neutral
    
    def _evaluate_code_generation(self, response: str, expected_keywords: List[str]) -> float:
        """Evaluate code generation"""
        response_lower = response.lower()
        
        found_keywords = sum(1 for keyword in expected_keywords if keyword.lower() in response_lower)
        return found_keywords / len(expected_keywords)
    
    def _evaluate_translation(self, response: str, expected: str) -> float:
        """Evaluate translation quality"""
        # Simple BLEU-like evaluation
        expected_words = set(expected.lower().split())
        response_words = set(response.lower().split())
        
        if not expected_words:
            return 0.0
        
        overlap = len(expected_words.intersection(response_words))
        precision = overlap / len(response_words) if response_words else 0.0
        recall = overlap / len(expected_words)
        
        if precision + recall == 0:
            return 0.0
        
        f1 = 2 * (precision * recall) / (precision + recall)
        return f1
    
    def run_full_evaluation(self) -> Dict[str, Any]:
        """
        Run complete Russian NLP evaluation suite
        """
        print("ğŸš€ Starting RADON Russian NLP Evaluation Suite")
        print("=" * 60)
        
        if not self.load_model():
            return {"error": "Failed to load model"}
        
        start_time = time.time()
        
        # Run all evaluations
        self.results = {
            "russian_superglue": self.evaluate_russian_superglue(),
            "code_generation": self.evaluate_code_generation(),
            "translation": self.evaluate_translation(),
            "long_context": self.evaluate_long_context()
        }
        
        # Calculate overall score
        all_scores = []
        for task_results in self.results.values():
            if isinstance(task_results, dict) and "average" in task_results:
                all_scores.append(task_results["average"])
        
        overall_score = np.mean(all_scores) if all_scores else 0.0
        
        # Add metadata
        self.results["metadata"] = {
            "model_name": self.model_name,
            "evaluation_time": time.time() - start_time,
            "overall_score": overall_score,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
        }
        
        print(f"\nğŸ¯ Overall RADON Score: {overall_score:.3f}")
        print(f"â±ï¸  Evaluation Time: {self.results['metadata']['evaluation_time']:.2f}s")
        
        return self.results
    
    def save_results(self, output_path: str = "results/russian_nlp_results.json"):
        """Save evaluation results"""
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ Results saved to {output_path}")


def main():
    """Run Russian NLP benchmark suite"""
    suite = RussianNLPSuite()
    results = suite.run_full_evaluation()
    suite.save_results()
    
    return results


if __name__ == "__main__":
    main()
