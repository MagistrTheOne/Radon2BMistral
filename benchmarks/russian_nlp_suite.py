"""
Russian NLP Benchmark Suite for RADON
Comprehensive evaluation on Russian language tasks
"""

import os
import json
import time
import torch
import numpy as np
from typing import Dict, List, Any, Optional, Tuple
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset
import evaluate
from pathlib import Path


class RussianNLPSuite:
    """
    Comprehensive benchmark suite for Russian NLP tasks
    
    Tasks included:
    - Russian SuperGLUE
    - Russian Code Generation
    - Multilingual Translation (RU-EN)
    - Russian Text Classification
    - Russian Named Entity Recognition
    """
    
    def __init__(self, model_name: str = "MagistrTheOne/RadonSAI-Pretrained"):
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.results = {}
        
        # Load metrics
        self.accuracy_metric = evaluate.load("accuracy")
        self.bleu_metric = evaluate.load("bleu")
        self.rouge_metric = evaluate.load("rouge")
        self.f1_metric = evaluate.load("f1")
    
    def load_model(self):
        """Load RADON model and tokenizer"""
        print(f"Loading RADON model: {self.model_name}")
        
        try:
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                torch_dtype=torch.float16,
                device_map="auto"
            )
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            
            # Set pad token if not set
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            print("‚úÖ Model loaded successfully")
            return True
            
        except Exception as e:
            print(f"‚ùå Failed to load model: {e}")
            return False
    
    def evaluate_russian_superglue(self) -> Dict[str, float]:
        """
        Evaluate on Russian SuperGLUE tasks
        """
        print("\nüìä Evaluating Russian SuperGLUE...")
        
        # Sample Russian SuperGLUE tasks
        tasks = {
            "russian_qa": {
                "question": "–ß—Ç–æ —Ç–∞–∫–æ–µ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ?",
                "context": "–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ - —ç—Ç–æ –ø–æ–¥—Ä–∞–∑–¥–µ–ª –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –∫–æ–º–ø—å—é—Ç–µ—Ä–∞–º –æ–±—É—á–∞—Ç—å—Å—è –∏ –ø—Ä–∏–Ω–∏–º–∞—Ç—å —Ä–µ—à–µ–Ω–∏—è –±–µ–∑ —è–≤–Ω–æ–≥–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è.",
                "expected_answer": "–ø–æ–¥—Ä–∞–∑–¥–µ–ª –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞"
            },
            "russian_sentiment": {
                "text": "–≠—Ç–æ—Ç —Ñ–∏–ª—å–º –±—ã–ª –ø—Ä–æ—Å—Ç–æ –ø–æ—Ç—Ä—è—Å–∞—é—â–∏–º!",
                "expected_label": "positive"
            },
            "russian_nli": {
                "premise": "–°–æ–±–∞–∫–∞ –±–µ–∂–∏—Ç –ø–æ –ø–∞—Ä–∫—É",
                "hypothesis": "–ñ–∏–≤–æ—Ç–Ω–æ–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –Ω–∞ —É–ª–∏—Ü–µ",
                "expected_label": "entailment"
            }
        }
        
        results = {}
        
        for task_name, task_data in tasks.items():
            print(f"  Testing {task_name}...")
            
            # Generate response
            prompt = self._create_prompt(task_name, task_data)
            response = self._generate_response(prompt)
            
            # Evaluate based on task type
            if task_name == "russian_qa":
                score = self._evaluate_qa(response, task_data["expected_answer"])
            elif task_name == "russian_sentiment":
                score = self._evaluate_sentiment(response, task_data["expected_label"])
            elif task_name == "russian_nli":
                score = self._evaluate_nli(response, task_data["expected_label"])
            
            results[task_name] = score
            print(f"    Score: {score:.3f}")
        
        avg_score = np.mean(list(results.values()))
        results["average"] = avg_score
        
        print(f"üìà Russian SuperGLUE Average: {avg_score:.3f}")
        return results
    
    def evaluate_code_generation(self) -> Dict[str, float]:
        """
        Evaluate Russian code generation capabilities
        """
        print("\nüíª Evaluating Code Generation...")
        
        code_tasks = {
            "python_function": {
                "prompt": "–ù–∞–ø–∏—à–∏ —Ñ—É–Ω–∫—Ü–∏—é –Ω–∞ Python –¥–ª—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏ —Å–ø–∏—Å–∫–∞:",
                "expected_keywords": ["def", "sort", "return"]
            },
            "russian_comments": {
                "prompt": "–°–æ–∑–¥–∞–π –∫–ª–∞—Å—Å –Ω–∞ Python —Å —Ä—É—Å—Å–∫–∏–º–∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è–º–∏:",
                "expected_keywords": ["class", "#", "def"]
            },
            "data_processing": {
                "prompt": "–ù–∞–ø–∏—à–∏ –∫–æ–¥ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ:",
                "expected_keywords": ["import", "pandas", "numpy"]
            }
        }
        
        results = {}
        
        for task_name, task_data in code_tasks.items():
            print(f"  Testing {task_name}...")
            
            response = self._generate_response(task_data["prompt"])
            score = self._evaluate_code_generation(response, task_data["expected_keywords"])
            
            results[task_name] = score
            print(f"    Score: {score:.3f}")
        
        avg_score = np.mean(list(results.values()))
        results["average"] = avg_score
        
        print(f"üìà Code Generation Average: {avg_score:.3f}")
        return results
    
    def evaluate_translation(self) -> Dict[str, float]:
        """
        Evaluate Russian-English translation
        """
        print("\nüåç Evaluating Translation...")
        
        translation_pairs = [
            {
                "russian": "–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ - —ç—Ç–æ –±—É–¥—É—â–µ–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π",
                "english": "Machine learning is the future of technology"
            },
            {
                "russian": "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –ø–æ–º–æ–≥–∞–µ—Ç —Ä–µ—à–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏",
                "english": "Artificial intelligence helps solve complex problems"
            },
            {
                "russian": "–ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ –∏–º–∏—Ç–∏—Ä—É—é—Ç —Ä–∞–±–æ—Ç—É —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ –º–æ–∑–≥–∞",
                "english": "Neural networks mimic the work of the human brain"
            }
        ]
        
        results = {}
        
        for i, pair in enumerate(translation_pairs):
            print(f"  Testing translation {i+1}...")
            
            # Russian to English
            ru_to_en_prompt = f"–ü–µ—Ä–µ–≤–µ–¥–∏ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π: {pair['russian']}"
            ru_to_en_response = self._generate_response(ru_to_en_prompt)
            ru_to_en_score = self._evaluate_translation(ru_to_en_response, pair["english"])
            
            # English to Russian
            en_to_ru_prompt = f"–ü–µ—Ä–µ–≤–µ–¥–∏ –Ω–∞ —Ä—É—Å—Å–∫–∏–π: {pair['english']}"
            en_to_ru_response = self._generate_response(en_to_ru_prompt)
            en_to_ru_score = self._evaluate_translation(en_to_ru_response, pair["russian"])
            
            results[f"ru_to_en_{i+1}"] = ru_to_en_score
            results[f"en_to_ru_{i+1}"] = en_to_ru_score
            
            print(f"    RU‚ÜíEN: {ru_to_en_score:.3f}, EN‚ÜíRU: {en_to_ru_score:.3f}")
        
        avg_score = np.mean(list(results.values()))
        results["average"] = avg_score
        
        print(f"üìà Translation Average: {avg_score:.3f}")
        return results
    
    def evaluate_long_context(self) -> Dict[str, float]:
        """
        Evaluate long context understanding in Russian
        """
        print("\nüìö Evaluating Long Context...")
        
        # Create long Russian text
        long_text = """
        –ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –ø–æ–¥—Ä–∞–∑–¥–µ–ª –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–µ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∑–∞–¥–∞—á–∏ –±–µ–∑ —è–≤–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π. –í–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ –æ–Ω–∏ –ø–æ–ª–∞–≥–∞—é—Ç—Å—è –Ω–∞ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∏ –≤—ã–≤–æ–¥—ã, –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –∏–∑ –¥–∞–Ω–Ω—ã—Ö.

        –û–±–ª–∞—Å—Ç—å –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Ç–µ—Å–Ω–æ —Å–≤—è–∑–∞–Ω–∞ —Å –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π, –∫–æ—Ç–æ—Ä–∞—è —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫–æ–º–ø—å—é—Ç–µ—Ä–æ–≤. –ò–∑—É—á–µ–Ω–∏–µ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥—ã, —Ç–µ–æ—Ä–∏—é –∏ –æ–±–ª–∞—Å—Ç–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –≤ –æ–±–ª–∞—Å—Ç–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.

        –ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏–Ω–æ–≥–¥–∞ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç—Å—è —Å –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º –¥–∞–Ω–Ω—ã—Ö, –≥–¥–µ –ø–æ—Å–ª–µ–¥–Ω—è—è —Å—É–±–¥–∏—Å—Ü–∏–ø–ª–∏–Ω–∞ –±–æ–ª—å—à–µ —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–º –∞–Ω–∞–ª–∏–∑–µ –¥–∞–Ω–Ω—ã—Ö –∏ –∏–∑–≤–µ—Å—Ç–Ω–∞ –∫–∞–∫ –Ω–µ–∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–µ –æ–±—É—á–µ–Ω–∏–µ. –ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Ç–∞–∫–∂–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –∏–∑–≤–µ—Å—Ç–Ω–æ –∫–∞–∫ –ø—Ä–æ–≥–Ω–æ–∑–Ω–∞—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∞.

        –°—É—â–µ—Å—Ç–≤—É–µ—Ç —Ç—Ä–∏ –æ—Å–Ω–æ–≤–Ω—ã—Ö —Ç–∏–ø–∞ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è: –æ–±—É—á–µ–Ω–∏–µ —Å —É—á–∏—Ç–µ–ª–µ–º, –æ–±—É—á–µ–Ω–∏–µ –±–µ–∑ —É—á–∏—Ç–µ–ª—è –∏ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –û–±—É—á–µ–Ω–∏–µ —Å —É—á–∏—Ç–µ–ª–µ–º –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–æ–º–µ—á–µ–Ω–Ω—ã–µ –æ–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∏–∑—É—á–µ–Ω–∏—è —Ñ—É–Ω–∫—Ü–∏–∏ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –æ—Ç –≤—Ö–æ–¥–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –∫ –≤—ã—Ö–æ–¥–Ω—ã–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º.

        –û–±—É—á–µ–Ω–∏–µ –±–µ–∑ —É—á–∏—Ç–µ–ª—è –Ω–∞—Ö–æ–¥–∏—Ç —Å–∫—Ä—ã—Ç—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≤ –¥–∞–Ω–Ω—ã—Ö –±–µ–∑ –ø–æ–º–µ—á–µ–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤. –û–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å –æ—Ç –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –æ–∫—Ä—É–∂–∞—é—â–µ–π —Å—Ä–µ–¥–æ–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.
        """
        
        questions = [
            "–ö–∞–∫–∏–µ —Ç—Ä–∏ —Ç–∏–ø–∞ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —É–ø–æ–º–∏–Ω–∞—é—Ç—Å—è –≤ —Ç–µ–∫—Å—Ç–µ?",
            "–ß—Ç–æ —Ç–∞–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å —É—á–∏—Ç–µ–ª–µ–º?",
            "–ö–∞–∫ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å–≤—è–∑–∞–Ω–æ —Å –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π?"
        ]
        
        results = {}
        
        for i, question in enumerate(questions):
            print(f"  Testing question {i+1}...")
            
            prompt = f"–ö–æ–Ω—Ç–µ–∫—Å—Ç: {long_text}\n\n–í–æ–ø—Ä–æ—Å: {question}\n\n–û—Ç–≤–µ—Ç:"
            response = self._generate_response(prompt)
            
            # Simple evaluation based on keyword presence
            score = self._evaluate_qa(response, question)
            results[f"question_{i+1}"] = score
            
            print(f"    Score: {score:.3f}")
        
        avg_score = np.mean(list(results.values()))
        results["average"] = avg_score
        
        print(f"üìà Long Context Average: {avg_score:.3f}")
        return results
    
    def _create_prompt(self, task_name: str, task_data: Dict) -> str:
        """Create prompt for specific task"""
        if task_name == "russian_qa":
            return f"–ö–æ–Ω—Ç–µ–∫—Å—Ç: {task_data['context']}\n\n–í–æ–ø—Ä–æ—Å: {task_data['question']}\n\n–û—Ç–≤–µ—Ç:"
        elif task_name == "russian_sentiment":
            return f"–û–ø—Ä–µ–¥–µ–ª–∏ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞: {task_data['text']}\n\n–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å:"
        elif task_name == "russian_nli":
            return f"–ü–æ—Å—ã–ª–∫–∞: {task_data['premise']}\n–ì–∏–ø–æ—Ç–µ–∑–∞: {task_data['hypothesis']}\n\n–û—Ç–Ω–æ—à–µ–Ω–∏–µ:"
        else:
            return str(task_data)
    
    def _generate_response(self, prompt: str, max_length: int = 100) -> str:
        """Generate response using RADON model"""
        if self.model is None or self.tokenizer is None:
            return ""
        
        try:
            inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512)
            inputs = {k: v.to(self.model.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_length=inputs["input_ids"].shape[1] + max_length,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )
            
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            # Remove the original prompt from response
            if response.startswith(prompt):
                response = response[len(prompt):].strip()
            
            return response
            
        except Exception as e:
            print(f"Generation error: {e}")
            return ""
    
    def _evaluate_qa(self, response: str, expected: str) -> float:
        """Evaluate QA response"""
        # Simple keyword-based evaluation
        expected_words = set(expected.lower().split())
        response_words = set(response.lower().split())
        
        if not expected_words:
            return 0.0
        
        overlap = len(expected_words.intersection(response_words))
        return overlap / len(expected_words)
    
    def _evaluate_sentiment(self, response: str, expected: str) -> float:
        """Evaluate sentiment classification"""
        response_lower = response.lower()
        
        if expected == "positive":
            positive_words = ["–ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π", "—Ö–æ—Ä–æ—à–∏–π", "–æ—Ç–ª–∏—á–Ω—ã–π", "–ø–æ—Ç—Ä—è—Å–∞—é—â–∏–π", "–∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω—ã–π"]
            return 1.0 if any(word in response_lower for word in positive_words) else 0.0
        elif expected == "negative":
            negative_words = ["–Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π", "–ø–ª–æ—Ö–æ–π", "—É–∂–∞—Å–Ω—ã–π", "–æ—Ç–≤—Ä–∞—Ç–∏—Ç–µ–ª—å–Ω—ã–π"]
            return 1.0 if any(word in response_lower for word in negative_words) else 0.0
        else:
            return 0.5  # Neutral
    
    def _evaluate_nli(self, response: str, expected: str) -> float:
        """Evaluate NLI response"""
        response_lower = response.lower()
        
        if expected == "entailment":
            entailment_words = ["—Å–ª–µ–¥—É–µ—Ç", "–ø–æ–¥—Ä–∞–∑—É–º–µ–≤–∞–µ—Ç", "–≤–ª–µ—á–µ—Ç", "entailment"]
            return 1.0 if any(word in response_lower for word in entailment_words) else 0.0
        elif expected == "contradiction":
            contradiction_words = ["–ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—Ç", "contradiction", "–Ω–µ–≤–µ—Ä–Ω–æ"]
            return 1.0 if any(word in response_lower for word in contradiction_words) else 0.0
        else:
            return 0.5  # Neutral
    
    def _evaluate_code_generation(self, response: str, expected_keywords: List[str]) -> float:
        """Evaluate code generation"""
        response_lower = response.lower()
        
        found_keywords = sum(1 for keyword in expected_keywords if keyword.lower() in response_lower)
        return found_keywords / len(expected_keywords)
    
    def _evaluate_translation(self, response: str, expected: str) -> float:
        """Evaluate translation quality"""
        # Simple BLEU-like evaluation
        expected_words = set(expected.lower().split())
        response_words = set(response.lower().split())
        
        if not expected_words:
            return 0.0
        
        overlap = len(expected_words.intersection(response_words))
        precision = overlap / len(response_words) if response_words else 0.0
        recall = overlap / len(expected_words)
        
        if precision + recall == 0:
            return 0.0
        
        f1 = 2 * (precision * recall) / (precision + recall)
        return f1
    
    def run_full_evaluation(self) -> Dict[str, Any]:
        """
        Run complete Russian NLP evaluation suite
        """
        print("üöÄ Starting RADON Russian NLP Evaluation Suite")
        print("=" * 60)
        
        if not self.load_model():
            return {"error": "Failed to load model"}
        
        start_time = time.time()
        
        # Run all evaluations
        self.results = {
            "russian_superglue": self.evaluate_russian_superglue(),
            "code_generation": self.evaluate_code_generation(),
            "translation": self.evaluate_translation(),
            "long_context": self.evaluate_long_context()
        }
        
        # Calculate overall score
        all_scores = []
        for task_results in self.results.values():
            if isinstance(task_results, dict) and "average" in task_results:
                all_scores.append(task_results["average"])
        
        overall_score = np.mean(all_scores) if all_scores else 0.0
        
        # Add metadata
        self.results["metadata"] = {
            "model_name": self.model_name,
            "evaluation_time": time.time() - start_time,
            "overall_score": overall_score,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
        }
        
        print(f"\nüéØ Overall RADON Score: {overall_score:.3f}")
        print(f"‚è±Ô∏è  Evaluation Time: {self.results['metadata']['evaluation_time']:.2f}s")
        
        return self.results
    
    def save_results(self, output_path: str = "results/russian_nlp_results.json"):
        """Save evaluation results"""
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        
        print(f"üíæ Results saved to {output_path}")


def main():
    """Run Russian NLP benchmark suite"""
    suite = RussianNLPSuite()
    results = suite.run_full_evaluation()
    suite.save_results()
    
    return results


if __name__ == "__main__":
    main()
