"""
Upload RADON datasets to Hugging Face Hub
"""

import os
import json
import shutil
from pathlib import Path
from huggingface_hub import HfApi, create_repo
from datasets import Dataset as HFDataset
import pandas as pd


def prepare_datasets_for_hf(
    data_dir: str = "./data",
    output_dir: str = "./hf_datasets"
):
    """–ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞—Ç–∞—Å–µ—Ç—ã –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –Ω–∞ HF"""
    
    print("üìä Preparing RADON datasets for Hugging Face Hub...")
    
    # –°–æ–∑–¥–∞–µ–º –≤—ã—Ö–æ–¥–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
    os.makedirs(output_dir, exist_ok=True)
    
    datasets = {}
    
    # 1. –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∫–æ—Ä–ø—É—Å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    print("[1/4] Preparing training corpus...")
    if os.path.exists(os.path.join(data_dir, "raw_corpus")):
        corpus_files = []
        for file in os.listdir(os.path.join(data_dir, "raw_corpus")):
            if file.endswith(('.txt', '.json')):
                corpus_files.append(os.path.join(data_dir, "raw_corpus", file))
        
        if corpus_files:
            # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç –∫–æ—Ä–ø—É—Å–∞
            corpus_data = []
            for file_path in corpus_files:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    corpus_data.append({
                        "text": content,
                        "source_file": os.path.basename(file_path),
                        "language": "mixed" if "combined" in file_path else "russian" if "russian" in file_path else "english"
                    })
            
            datasets["radon-corpus"] = {
                "data": corpus_data,
                "description": "RADON training corpus with Russian, English, and code samples",
                "tags": ["russian", "english", "code", "training", "corpus"]
            }
    
    # 2. –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã
    print("[2/4] Preparing test datasets...")
    test_datasets = {
        "multilingual": {
            "prompts": [
                "–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ - —ç—Ç–æ",
                "Machine learning is",
                "def train_model():",
                "–°–æ–∑–¥–∞–π –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å –¥–ª—è",
                "Implement a function that"
            ],
            "expected_topics": ["ML", "programming", "AI", "neural networks"]
        },
        "long_context": {
            "prompts": [
                "–í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ –º—ã —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º –æ—Å–Ω–æ–≤—ã –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ –µ–≥–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö. –ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ - —ç—Ç–æ –ø–æ–¥—Ä–∞–∑–¥–µ–ª –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –∫–æ–º–ø—å—é—Ç–µ—Ä–∞–º –æ–±—É—á–∞—Ç—å—Å—è –∏ –ø—Ä–∏–Ω–∏–º–∞—Ç—å —Ä–µ—à–µ–Ω–∏—è –±–µ–∑ —è–≤–Ω–æ–≥–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è. –°—É—â–µ—Å—Ç–≤—É–µ—Ç —Ç—Ä–∏ –æ—Å–Ω–æ–≤–Ω—ã—Ö —Ç–∏–ø–∞ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è: –æ–±—É—á–µ–Ω–∏–µ —Å —É—á–∏—Ç–µ–ª–µ–º, –æ–±—É—á–µ–Ω–∏–µ –±–µ–∑ —É—á–∏—Ç–µ–ª—è –∏ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –ö–∞–∂–¥—ã–π –∏–∑ —ç—Ç–∏—Ö —Ç–∏–ø–æ–≤ –∏–º–µ–µ—Ç —Å–≤–æ–∏ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –∏ –æ–±–ª–∞—Å—Ç–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è.",
                "The following is a comprehensive guide to machine learning algorithms and their implementations. We will cover supervised learning, unsupervised learning, and reinforcement learning approaches. Each method has its strengths and weaknesses, and the choice depends on the specific problem domain and available data."
            ],
            "context_length": "long"
        },
        "code_generation": {
            "prompts": [
                "def calculate_loss(y_true, y_pred):",
                "class NeuralNetwork:",
                "import torch.nn as nn",
                "def train_epoch(model, dataloader, optimizer):",
                "def evaluate_model(model, test_data):"
            ],
            "language": "python"
        }
    }
    
    for name, data in test_datasets.items():
        datasets[f"radon-test-{name}"] = {
            "data": [{"prompt": prompt, "category": name} for prompt in data["prompts"]],
            "description": f"RADON test dataset for {name} evaluation",
            "tags": ["test", "evaluation", name]
        }
    
    # 3. –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    print("[3/4] Preparing usage examples...")
    usage_examples = [
        {
            "prompt": "–û–±—ä—è—Å–Ω–∏, —á—Ç–æ —Ç–∞–∫–æ–µ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ",
            "expected_response": "–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ - —ç—Ç–æ –ø–æ–¥—Ä–∞–∑–¥–µ–ª –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞...",
            "category": "explanation",
            "language": "russian"
        },
        {
            "prompt": "Write a Python function to calculate accuracy",
            "expected_response": "def calculate_accuracy(y_true, y_pred):\n    return (y_true == y_pred).mean()",
            "category": "code_generation",
            "language": "english"
        },
        {
            "prompt": "–°–æ–∑–¥–∞–π –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π",
            "expected_response": "import torch.nn as nn\n\nclass ImageClassifier(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 32, 3)\n        # ...",
            "category": "code_generation",
            "language": "russian"
        }
    ]
    
    datasets["radon-examples"] = {
        "data": usage_examples,
        "description": "RADON usage examples and expected responses",
        "tags": ["examples", "usage", "prompts", "responses"]
    }
    
    # 4. –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞–¥–∞—Ç–∞—Å–µ—Ç —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –≤—Å–µ—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö
    print("[4/4] Creating metadata...")
    metadata = {
        "radon_datasets": {
            "description": "RADON Mistral-based transformer datasets collection",
            "version": "1.0.0",
            "created_by": "MagistrTheOne",
            "model": "MagistrTheOne/RadonSAI",
            "datasets": list(datasets.keys()),
            "total_examples": sum(len(ds["data"]) for ds in datasets.values()),
            "languages": ["russian", "english", "mixed"],
            "categories": ["training", "testing", "examples", "corpus"]
        }
    }
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ –¥–∞—Ç–∞—Å–µ—Ç—ã
    for name, dataset_info in datasets.items():
        dataset_path = os.path.join(output_dir, name)
        os.makedirs(dataset_path, exist_ok=True)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ
        with open(os.path.join(dataset_path, "data.json"), 'w', encoding='utf-8') as f:
            json.dump(dataset_info["data"], f, ensure_ascii=False, indent=2)
        
        # –°–æ–∑–¥–∞–µ–º README –¥–ª—è –¥–∞—Ç–∞—Å–µ—Ç–∞
        readme_content = f"""# {name}

## Description
{dataset_info['description']}

## Tags
{', '.join(dataset_info['tags'])}

## Usage
```python
from datasets import load_dataset

dataset = load_dataset("MagistrTheOne/{name}")
```

## Examples
```python
# Load and use the dataset
data = dataset['train']
for example in data:
    print(example)
```
"""
        
        with open(os.path.join(dataset_path, "README.md"), 'w', encoding='utf-8') as f:
            f.write(readme_content)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    with open(os.path.join(output_dir, "metadata.json"), 'w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)
    
    print(f"‚úÖ Datasets prepared in {output_dir}")
    return output_dir, datasets


def upload_dataset_to_hf(
    dataset_path: str,
    dataset_name: str,
    repo_id: str,
    hf_token: str = None
):
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å –æ—Ç–¥–µ–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –Ω–∞ HF"""
    
    print(f"üì§ Uploading {dataset_name} to Hugging Face...")
    
    if not hf_token:
        hf_token = os.getenv("HF_TOKEN")
        if not hf_token:
            raise ValueError("HF_TOKEN not found in environment variables")
    
    api = HfApi(token=hf_token)
    
    try:
        # –°–æ–∑–¥–∞–µ–º —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π –¥–ª—è –¥–∞—Ç–∞—Å–µ—Ç–∞
        full_repo_id = f"{repo_id}/{dataset_name}"
        create_repo(
            repo_id=full_repo_id,
            repo_type="dataset",
            private=False,
            exist_ok=True,
            token=hf_token
        )
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–π–ª—ã
        api.upload_folder(
            folder_path=dataset_path,
            repo_id=full_repo_id,
            repo_type="dataset",
            commit_message=f"üìä Upload {dataset_name} dataset"
        )
        
        print(f"‚úÖ {dataset_name} uploaded: https://huggingface.co/datasets/{full_repo_id}")
        return True
        
    except Exception as e:
        print(f"‚ùå Failed to upload {dataset_name}: {e}")
        return False


def upload_all_datasets(
    datasets_dir: str,
    base_repo_id: str = "MagistrTheOne",
    hf_token: str = None
):
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å –≤—Å–µ –¥–∞—Ç–∞—Å–µ—Ç—ã –Ω–∞ HF"""
    
    print("üöÄ Uploading all RADON datasets to Hugging Face Hub...")
    
    if not hf_token:
        hf_token = os.getenv("HF_TOKEN")
        if not hf_token:
            raise ValueError("HF_TOKEN not found in environment variables")
    
    success_count = 0
    total_count = 0
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–∞–∂–¥—ã–π –¥–∞—Ç–∞—Å–µ—Ç
    for item in os.listdir(datasets_dir):
        item_path = os.path.join(datasets_dir, item)
        if os.path.isdir(item_path) and item != "metadata.json":
            total_count += 1
            success = upload_dataset_to_hf(
                dataset_path=item_path,
                dataset_name=item,
                repo_id=base_repo_id,
                hf_token=hf_token
            )
            if success:
                success_count += 1
    
    print(f"\nüìä Upload Summary:")
    print(f"   ‚úÖ Successful: {success_count}/{total_count}")
    print(f"   ‚ùå Failed: {total_count - success_count}/{total_count}")
    
    return success_count == total_count


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    
    print("üìä RADON Datasets Upload to Hugging Face")
    print("=" * 50)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–∫–µ–Ω
    hf_token = os.getenv("HF_TOKEN")
    if not hf_token:
        print("‚ùå HF_TOKEN not found in environment variables")
        print("   Set it with: $env:HF_TOKEN='your_token_here'")
        return
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç—ã
    output_dir, datasets = prepare_datasets_for_hf()
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ –¥–∞—Ç–∞—Å–µ—Ç—ã
    success = upload_all_datasets(
        datasets_dir=output_dir,
        base_repo_id="MagistrTheOne",
        hf_token=hf_token
    )
    
    if success:
        print("\n‚úÖ All datasets successfully uploaded!")
        print("üîó Available datasets:")
        for name in datasets.keys():
            print(f"   üìä https://huggingface.co/datasets/MagistrTheOne/{name}")
    else:
        print("\n‚ö†Ô∏è  Some datasets failed to upload")


if __name__ == "__main__":
    main()
