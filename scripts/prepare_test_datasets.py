"""
–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ RADON
"""

import os
import json
import random
from pathlib import Path
from typing import List, Dict, Any


def create_russian_ml_corpus(output_file: str, num_samples: int = 1000):
    """–°–æ–∑–¥–∞—Ç—å —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–π ML –∫–æ—Ä–ø—É—Å"""
    
    russian_ml_texts = [
        "–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ - —ç—Ç–æ –ø–æ–¥—Ä–∞–∑–¥–µ–ª –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –∞–ª–≥–æ—Ä–∏—Ç–º–∞—Ö.",
        "–ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ —Å–æ—Å—Ç–æ—è—Ç –∏–∑ —Å–ª–æ–µ–≤ –Ω–µ–π—Ä–æ–Ω–æ–≤, —Å–æ–µ–¥–∏–Ω–µ–Ω–Ω—ã—Ö –≤–µ—Å–∞–º–∏.",
        "–ì–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–Ω–æ–≥–æ—Å–ª–æ–π–Ω—ã–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á.",
        "–û–±—É—á–µ–Ω–∏–µ —Å —É—á–∏—Ç–µ–ª–µ–º —Ç—Ä–µ–±—É–µ—Ç —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏.",
        "–û–±—É—á–µ–Ω–∏–µ –±–µ–∑ —É—á–∏—Ç–µ–ª—è –Ω–∞—Ö–æ–¥–∏—Ç —Å–∫—Ä—ã—Ç—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≤ –¥–∞–Ω–Ω—ã—Ö –±–µ–∑ –º–µ—Ç–æ–∫.",
        "–°–≤–µ—Ä—Ç–æ—á–Ω—ã–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.",
        "–†–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ –ø–æ–¥—Ö–æ–¥—è—Ç –¥–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.",
        "–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π.",
        "–ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ - —ç—Ç–æ –∞–ª–≥–æ—Ä–∏—Ç–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π.",
        "–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –ø–æ–º–æ–≥–∞–µ—Ç –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—Ç–∏—Ç—å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏.",
        "–ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ—Ü–µ–Ω–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏ –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö.",
        "–ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –≤–∫–ª—é—á–∞—é—Ç —Ç–æ—á–Ω–æ—Å—Ç—å, –ø–æ–ª–Ω–æ—Ç—É –∏ F1-–º–µ—Ä—É.",
        "–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö - –≤–∞–∂–Ω—ã–π —ç—Ç–∞–ø –≤ –º–∞—à–∏–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏.",
        "Feature engineering - —Å–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.",
        "Ensemble –º–µ—Ç–æ–¥—ã –∫–æ–º–±–∏–Ω–∏—Ä—É—é—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–æ–¥–µ–ª–µ–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞.",
        "–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞—é—Ç—Å—è –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.",
        "Overfitting –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç, –∫–æ–≥–¥–∞ –º–æ–¥–µ–ª—å –∑–∞–ø–æ–º–∏–Ω–∞–µ—Ç –æ–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ.",
        "Underfitting –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ –º–æ–¥–µ–ª—å —Å–ª–∏—à–∫–æ–º –ø—Ä–æ—Å—Ç–∞—è –¥–ª—è –¥–∞–Ω–Ω—ã—Ö.",
        "Bias-variance tradeoff - –∫–æ–º–ø—Ä–æ–º–∏—Å—Å –º–µ–∂–¥—É —Å–º–µ—â–µ–Ω–∏–µ–º –∏ –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π.",
        "–î–∞–Ω–Ω—ã–µ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Ä–µ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ç–∏–≤–Ω—ã–º–∏ –¥–ª—è —Ü–µ–ª–µ–≤–æ–π –∑–∞–¥–∞—á–∏."
    ]
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤–∞—Ä–∏–∞—Ü–∏–∏
    generated_texts = []
    for _ in range(num_samples):
        base_text = random.choice(russian_ml_texts)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤–∞—Ä–∏–∞—Ü–∏–∏
        variations = [
            f"–í –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ {base_text.lower()}",
            f"–í–∞–∂–Ω–æ –æ—Ç–º–µ—Ç–∏—Ç—å, —á—Ç–æ {base_text.lower()}",
            f"–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ {base_text.lower()}",
            f"–í –æ–±–ª–∞—Å—Ç–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è {base_text.lower()}",
            f"–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –æ–ø—ã—Ç –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ {base_text.lower()}"
        ]
        
        generated_texts.append(random.choice(variations))
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    with open(output_file, 'w', encoding='utf-8') as f:
        for text in generated_texts:
            f.write(text + '\n\n')
    
    print(f"‚úÖ Created Russian ML corpus: {len(generated_texts)} samples")
    return len(generated_texts)


def create_english_tech_corpus(output_file: str, num_samples: int = 1000):
    """–°–æ–∑–¥–∞—Ç—å –∞–Ω–≥–ª–æ—è–∑—ã—á–Ω—ã–π tech –∫–æ—Ä–ø—É—Å"""
    
    english_tech_texts = [
        "Machine learning algorithms can be supervised, unsupervised, or reinforcement learning.",
        "Deep learning models require large amounts of data and computational resources.",
        "Neural networks are inspired by biological neural networks in the brain.",
        "Convolutional neural networks are particularly effective for image recognition tasks.",
        "Recurrent neural networks can process sequential data with memory.",
        "Transformers use self-attention mechanisms to process sequences efficiently.",
        "Natural language processing combines linguistics with machine learning.",
        "Computer vision enables machines to interpret and understand visual information.",
        "Data preprocessing is crucial for successful machine learning projects.",
        "Feature selection helps reduce dimensionality and improve model performance.",
        "Cross-validation provides a robust estimate of model generalization.",
        "Hyperparameter tuning optimizes model performance on validation data.",
        "Regularization techniques prevent overfitting in machine learning models.",
        "Ensemble methods combine multiple models to achieve better performance.",
        "Transfer learning leverages pre-trained models for new tasks.",
        "Model interpretability is important for understanding AI decisions.",
        "Ethical considerations are crucial in AI system development.",
        "Bias in training data can lead to unfair AI system outcomes.",
        "Explainable AI aims to make machine learning models more transparent.",
        "Continuous learning allows models to adapt to new data over time."
    ]
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤–∞—Ä–∏–∞—Ü–∏–∏
    generated_texts = []
    for _ in range(num_samples):
        base_text = random.choice(english_tech_texts)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤–∞—Ä–∏–∞—Ü–∏–∏
        variations = [
            f"In modern AI, {base_text.lower()}",
            f"Recent research demonstrates that {base_text.lower()}",
            f"Industry best practices suggest that {base_text.lower()}",
            f"Advanced techniques show that {base_text.lower()}",
            f"Practical applications reveal that {base_text.lower()}"
        ]
        
        generated_texts.append(random.choice(variations))
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    with open(output_file, 'w', encoding='utf-8') as f:
        for text in generated_texts:
            f.write(text + '\n\n')
    
    print(f"‚úÖ Created English tech corpus: {len(generated_texts)} samples")
    return len(generated_texts)


def create_code_corpus(output_file: str, num_samples: int = 500):
    """–°–æ–∑–¥–∞—Ç—å –∫–æ—Ä–ø—É—Å –∫–æ–¥–∞"""
    
    code_samples = [
        "def train_model(X, y, epochs=100):\n    model = Sequential()\n    model.add(Dense(64, activation='relu'))\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(optimizer='adam', loss='binary_crossentropy')\n    model.fit(X, y, epochs=epochs)\n    return model",
        
        "import torch\nimport torch.nn as nn\n\nclass Transformer(nn.Module):\n    def __init__(self, d_model, nhead):\n        super().__init__()\n        self.attention = nn.MultiheadAttention(d_model, nhead)\n        self.norm = nn.LayerNorm(d_model)\n    \n    def forward(self, x):\n        attn_output, _ = self.attention(x, x, x)\n        return self.norm(x + attn_output)",
        
        "def preprocess_text(text):\n    text = text.lower()\n    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n    tokens = text.split()\n    return [word for word in tokens if len(word) > 2]",
        
        "class DataLoader:\n    def __init__(self, dataset, batch_size=32):\n        self.dataset = dataset\n        self.batch_size = batch_size\n    \n    def __iter__(self):\n        for i in range(0, len(self.dataset), self.batch_size):\n            yield self.dataset[i:i+self.batch_size]",
        
        "def calculate_accuracy(predictions, targets):\n    correct = (predictions == targets).sum().item()\n    total = targets.size(0)\n    return correct / total",
        
        "def tokenize_text(text, tokenizer):\n    tokens = tokenizer.encode(text)\n    return torch.tensor(tokens).unsqueeze(0)",
        
        "def save_model(model, path):\n    torch.save(model.state_dict(), path)\n    print(f'Model saved to {path}')",
        
        "def load_model(model_class, path, **kwargs):\n    model = model_class(**kwargs)\n    model.load_state_dict(torch.load(path))\n    return model"
    ]
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤–∞—Ä–∏–∞—Ü–∏–∏
    generated_texts = []
    for _ in range(num_samples):
        base_code = random.choice(code_samples)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∏ –≤–∞—Ä–∏–∞—Ü–∏–∏
        variations = [
            f"# Machine learning implementation\n{base_code}",
            f"# Deep learning model\n{base_code}",
            f"# Data processing function\n{base_code}",
            f"# Neural network architecture\n{base_code}",
            f"# Training utility\n{base_code}"
        ]
        
        generated_texts.append(random.choice(variations))
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    with open(output_file, 'w', encoding='utf-8') as f:
        for text in generated_texts:
            f.write(text + '\n\n')
    
    print(f"‚úÖ Created code corpus: {len(generated_texts)} samples")
    return len(generated_texts)


def create_combined_corpus(
    russian_file: str,
    english_file: str, 
    code_file: str,
    output_file: str,
    russian_ratio: float = 0.4,
    english_ratio: float = 0.4,
    code_ratio: float = 0.2
):
    """–°–æ–∑–¥–∞—Ç—å –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π –∫–æ—Ä–ø—É—Å"""
    
    print("[+] Creating combined test corpus...")
    
    # –ß–∏—Ç–∞–µ–º –∫–æ—Ä–ø—É—Å—ã
    with open(russian_file, 'r', encoding='utf-8') as f:
        russian_texts = [line.strip() for line in f if line.strip()]
    
    with open(english_file, 'r', encoding='utf-8') as f:
        english_texts = [line.strip() for line in f if line.strip()]
    
    with open(code_file, 'r', encoding='utf-8') as f:
        code_texts = [line.strip() for line in f if line.strip()]
    
    # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
    total_samples = len(russian_texts) + len(english_texts) + len(code_texts)
    russian_count = int(total_samples * russian_ratio)
    english_count = int(total_samples * english_ratio)
    code_count = int(total_samples * code_ratio)
    
    # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π –∫–æ—Ä–ø—É—Å
    combined_texts = []
    combined_texts.extend(russian_texts[:russian_count])
    combined_texts.extend(english_texts[:english_count])
    combined_texts.extend(code_texts[:code_count])
    
    # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º
    random.shuffle(combined_texts)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    with open(output_file, 'w', encoding='utf-8') as f:
        for text in combined_texts:
            f.write(text + '\n\n')
    
    stats = {
        "total_samples": len(combined_texts),
        "russian_samples": russian_count,
        "english_samples": english_count,
        "code_samples": code_count,
        "total_chars": sum(len(text) for text in combined_texts)
    }
    
    print(f"‚úÖ Combined corpus: {stats['total_samples']} samples, {stats['total_chars']:,} chars")
    return stats


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    
    print("üß™ Preparing test datasets for RADON evaluation...")
    
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    os.makedirs("data/test_datasets", exist_ok=True)
    
    # –°–æ–∑–¥–∞–µ–º –∫–æ—Ä–ø—É—Å—ã
    russian_count = create_russian_ml_corpus("data/test_datasets/russian_ml.txt", 1000)
    english_count = create_english_tech_corpus("data/test_datasets/english_tech.txt", 1000)
    code_count = create_code_corpus("data/test_datasets/code_samples.txt", 500)
    
    # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π –∫–æ—Ä–ø—É—Å
    stats = create_combined_corpus(
        "data/test_datasets/russian_ml.txt",
        "data/test_datasets/english_tech.txt",
        "data/test_datasets/code_samples.txt",
        "data/test_datasets/combined_test.txt"
    )
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    with open("data/test_datasets/corpus_stats.json", 'w', encoding='utf-8') as f:
        json.dump(stats, f, indent=2, ensure_ascii=False)
    
    print("\nüéâ Test datasets ready!")
    print(f"üìä Total samples: {stats['total_samples']}")
    print(f"üìä Total characters: {stats['total_chars']:,}")
    print(f"üìÅ Files created in data/test_datasets/")


if __name__ == "__main__":
    main()
