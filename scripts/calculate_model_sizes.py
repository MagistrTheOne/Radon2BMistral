"""
Calculate model sizes for different RADON configurations
"""

import json
import torch


def calculate_model_size(
    vocab_size: int,
    hidden_size: int,
    num_layers: int,
    num_attention_heads: int,
    num_kv_heads: int,
    intermediate_size: int,
    max_position_embeddings: int,
    dtype: str = "float16"
):
    """–†–∞—Å—Å—á–∏—Ç–∞—Ç—å —Ä–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏"""
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞–∑–º–µ—Ä —ç–ª–µ–º–µ–Ω—Ç–∞ –¥–∞–Ω–Ω—ã—Ö
    dtype_sizes = {
        "float32": 4,
        "float16": 2,
        "bfloat16": 2,
        "int8": 1,
        "int4": 0.5
    }
    
    element_size = dtype_sizes.get(dtype, 2)
    
    # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    parameters = 0
    
    # 1. Embeddings
    embed_params = vocab_size * hidden_size
    parameters += embed_params
    print(f"üìä Embeddings: {embed_params:,} parameters")
    
    # 2. Layers
    for layer_idx in range(num_layers):
        layer_params = 0
        
        # Attention weights
        q_proj = hidden_size * hidden_size
        k_proj = num_kv_heads * (hidden_size // num_attention_heads) * hidden_size
        v_proj = num_kv_heads * (hidden_size // num_attention_heads) * hidden_size
        o_proj = hidden_size * hidden_size
        
        attention_params = q_proj + k_proj + v_proj + o_proj
        layer_params += attention_params
        
        # MLP weights
        gate_proj = intermediate_size * hidden_size
        up_proj = intermediate_size * hidden_size
        down_proj = hidden_size * intermediate_size
        
        mlp_params = gate_proj + up_proj + down_proj
        layer_params += mlp_params
        
        # Layer norms
        layer_norm_params = hidden_size * 2  # input_layernorm + post_attention_layernorm
        layer_params += layer_norm_params
        
        parameters += layer_params
        print(f"üìä Layer {layer_idx + 1}: {layer_params:,} parameters")
    
    # 3. Final layer norm
    final_norm_params = hidden_size
    parameters += final_norm_params
    print(f"üìä Final norm: {final_norm_params:,} parameters")
    
    # 4. LM head
    lm_head_params = vocab_size * hidden_size
    parameters += lm_head_params
    print(f"üìä LM head: {lm_head_params:,} parameters")
    
    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä—ã
    size_bytes = parameters * element_size
    size_mb = size_bytes / (1024 * 1024)
    size_gb = size_bytes / (1024 * 1024 * 1024)
    
    return {
        "parameters": parameters,
        "size_bytes": size_bytes,
        "size_mb": size_mb,
        "size_gb": size_gb,
        "dtype": dtype,
        "element_size": element_size
    }


def analyze_radon_configs():
    """–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–∑–º–µ—Ä—ã —Ä–∞–∑–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π RADON"""
    
    print("üîç RADON Model Size Analysis")
    print("=" * 50)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    configs = {
        "Small (50M)": {
            "vocab_size": 8192,
            "hidden_size": 512,
            "num_layers": 6,
            "num_attention_heads": 8,
            "num_kv_heads": 2,
            "intermediate_size": 1024,
            "max_position_embeddings": 2048
        },
        "Medium (500M)": {
            "vocab_size": 16384,
            "hidden_size": 1024,
            "num_layers": 12,
            "num_attention_heads": 16,
            "num_kv_heads": 4,
            "intermediate_size": 2048,
            "max_position_embeddings": 4096
        },
        "Large (2B)": {
            "vocab_size": 32000,
            "hidden_size": 2048,
            "num_layers": 24,
            "num_attention_heads": 32,
            "num_kv_heads": 8,
            "intermediate_size": 5632,
            "max_position_embeddings": 8192
        },
        "XL (7B)": {
            "vocab_size": 32000,
            "hidden_size": 4096,
            "num_layers": 32,
            "num_attention_heads": 32,
            "num_kv_heads": 8,
            "intermediate_size": 11008,
            "max_position_embeddings": 16384
        }
    }
    
    results = {}
    
    for config_name, config in configs.items():
        print(f"\nüìä {config_name} Configuration:")
        print("-" * 30)
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö
        for dtype in ["float16", "float32", "int8", "int4"]:
            result = calculate_model_size(dtype=dtype, **config)
            results[f"{config_name}_{dtype}"] = result
            
            print(f"   {dtype.upper()}: {result['parameters']:,} params, {result['size_gb']:.2f} GB")
    
    return results


def estimate_training_requirements():
    """–û—Ü–µ–Ω–∏—Ç—å —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏"""
    
    print("\nüöÄ Training Requirements Estimation")
    print("=" * 50)
    
    # –ü—Ä–∏–º–µ—Ä–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏
    training_multipliers = {
        "optimizer_states": 2,  # AdamW —Ö—Ä–∞–Ω–∏—Ç momentum –∏ variance
        "gradients": 1,         # –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã
        "activations": 0.5,     # –ê–∫—Ç–∏–≤–∞—Ü–∏–∏ (–∑–∞–≤–∏—Å–∏—Ç –æ—Ç batch size)
        "mixed_precision": 0.5, # FP16 training
        "gradient_checkpointing": 0.3  # –≠–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏
    }
    
    configs = {
        "2B Model": {
            "parameters": 2_000_000_000,
            "base_memory_gb": 4.0  # FP16
        },
        "7B Model": {
            "parameters": 7_000_000_000,
            "base_memory_gb": 14.0  # FP16
        }
    }
    
    for model_name, config in configs.items():
        print(f"\nüìä {model_name} Training Requirements:")
        
        base_memory = config["base_memory_gb"]
        
        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞
        standard_memory = base_memory * (1 + training_multipliers["optimizer_states"] + training_multipliers["gradients"])
        print(f"   Standard training: {standard_memory:.1f} GB")
        
        # –° mixed precision
        mixed_precision_memory = base_memory * (1 + training_multipliers["optimizer_states"] + training_multipliers["gradients"] + training_multipliers["mixed_precision"])
        print(f"   Mixed precision: {mixed_precision_memory:.1f} GB")
        
        # –° gradient checkpointing
        checkpointing_memory = base_memory * (1 + training_multipliers["optimizer_states"] + training_multipliers["gradients"] + training_multipliers["gradient_checkpointing"])
        print(f"   + Gradient checkpointing: {checkpointing_memory:.1f} GB")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ GPU
        print(f"   Recommended GPUs:")
        if standard_memory <= 24:
            print(f"     - RTX 4090 (24GB): ‚úÖ")
        if standard_memory <= 40:
            print(f"     - A100 (40GB): ‚úÖ")
        if standard_memory <= 80:
            print(f"     - A100 (80GB): ‚úÖ")
        if standard_memory > 80:
            print(f"     - Multi-GPU setup required")


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–∞–∑–º–µ—Ä—ã
    results = analyze_radon_configs()
    
    # –û—Ü–µ–Ω–∏–≤–∞–µ–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏
    estimate_training_requirements()
    
    print("\nüìã Summary:")
    print("=" * 50)
    print("üîπ Small (50M): ~100MB - –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
    print("üîπ Medium (500M): ~1GB - –¥–ª—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤")
    print("üîπ Large (2B): ~4GB - –æ—Å–Ω–æ–≤–Ω–∞—è –º–æ–¥–µ–ª—å")
    print("üîπ XL (7B): ~14GB - –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å")
    
    print("\nüí° Recommendations:")
    print("üîπ RTX 2080 (8GB): Small + Medium –º–æ–¥–µ–ª–∏")
    print("üîπ RTX 4070 (12GB): Medium + Large –º–æ–¥–µ–ª–∏")
    print("üîπ RTX 4090 (24GB): Large + XL –º–æ–¥–µ–ª–∏")
    print("üîπ A100 (40GB+): –í—Å–µ –º–æ–¥–µ–ª–∏ + —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞")


if __name__ == "__main__":
    main()
